{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review, stem):\n",
    "    \"\"\"Clean a given movie review\n",
    "    \n",
    "    Tokenizes and removes punctuation and additional styling/formatting\n",
    "    \n",
    "    Args:\n",
    "        review (str): movie review text string\n",
    "        stem (boolean): indicator whether to stem or just tokenize\n",
    "        \n",
    "    Returns:\n",
    "        clean_review (list): tokenized, cleaned movie review\n",
    "    \"\"\"\n",
    "    # Remove styling\n",
    "    review = review.replace(\"--\", \"\").replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    review = word_tokenize(review)\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        review = [stemmer.stem(token) for token in review]\n",
    "    # Remove punctuation\n",
    "    cleaned_review = [x for x in review if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews(dir_path, stem=False):\n",
    "    \"\"\"Reads in all reviews from directory and performs train/test split\n",
    "    \n",
    "    Args:\n",
    "        dir_path (str): path to directory containing reviews\n",
    "        stem (boolean): indicator whether to stem or just tokenize\n",
    "        \n",
    "    Returns:\n",
    "        train (list): list of cleaned, tokenized movie reviews for training\n",
    "        test (list): list of cleaned, tokenized movie reviews for testing\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "    for filename in os.listdir(dir_path):\n",
    "        with open(os.path.join(dir_path, filename), 'r') as f:\n",
    "            review = clean_review(f.read(), stem)\n",
    "            # Files beginning with cv9 indicate that it's part of the test set (cross-validation fold 9)\n",
    "            if filename.startswith('cv9'):\n",
    "                test.append(review)\n",
    "            else:\n",
    "                train.append(review)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_adv(reviews):\n",
    "    \"\"\"Filters out words that aren't adjective or adverbs from review\n",
    "    \n",
    "    Args:\n",
    "        reviews (list): list of cleaned, tokenized movie reviews\n",
    "        \n",
    "    Returns:\n",
    "        reviews_adj_adv (list): list of cleaned, tokenized movie reviews with only adverbs/adjectives\n",
    "    \"\"\"\n",
    "    reviews_adj_adv = []\n",
    "    # POS tags for adjectives and adverbs\n",
    "    adj_adv = ['JJ','JJR','JJS','RB','RBR','RBS']\n",
    "    for review in reviews:\n",
    "        review_pos = pos_tag(review)\n",
    "        review_adj_adv = [word[0] for word in review_pos if word[1] in adj_adv]\n",
    "        reviews_adj_adv.append(review_adj_adv)\n",
    "    return reviews_adj_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pos_neg(review, NRC_emotion):\n",
    "    \"\"\"Calculates the positive/negative review ratio from NRC_emotion\n",
    "    \n",
    "    Args:\n",
    "        review (list): tokenized, cleaned movie review\n",
    "        NRC_emotion (df): dataframe of word, affect, and indicator\n",
    "    \n",
    "    Returns:\n",
    "        ratio (float): positive/negative ratio given tokens in review\n",
    "    \"\"\"\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for token in review:\n",
    "        if token in NRC_emotion.Word.values:\n",
    "            word_emotion = NRC_emotion[NRC_emotion.Word == token]\n",
    "            # Increase count if word has a flag for positive or negative\n",
    "            if \"positive\" in word_emotion.Affect.values:\n",
    "                pos += 1\n",
    "            if \"negative\" in word_emotion.Affect.values:\n",
    "                neg += 1\n",
    "    if neg != 0:\n",
    "        ratio = pos/neg\n",
    "    else:\n",
    "        ratio = pos\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and preprocess data\n",
    "\n",
    "Since the data has already been tokenized and downcased, our preprocessing includes removing styling (as seen through words surrounded by `_` or `-` in visual inspection) and removing punctuation. For some experiments, words are also stemmed and/or stop words are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train, neg_test = read_reviews('review_polarity.v2/txt_sentoken/neg')\n",
    "pos_train, pos_test = read_reviews('review_polarity.v2/txt_sentoken/pos')\n",
    "pos_train_labels = [1 for i in range(len(pos_train))]\n",
    "neg_train_labels = [0 for i in range(len(neg_train))]\n",
    "pos_test_labels = [1 for i in range(len(pos_test))]\n",
    "neg_test_labels = [0 for i in range(len(neg_test))]\n",
    "train = pos_train + neg_train\n",
    "test = pos_test + neg_test\n",
    "train_labels = pos_train_labels + neg_train_labels\n",
    "test_labels = pos_test_labels + neg_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "## M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 900.0 the\n",
      "1 900.0 of\n",
      "1 899.0 to\n",
      "1 899.0 is\n",
      "1 899.0 and\n",
      "1 898.0 in\n",
      "1 891.0 it\n",
      "1 887.0 that\n",
      "1 879.0 with\n",
      "1 873.0 as\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 899.0 the\n",
      "0 899.0 of\n",
      "0 899.0 and\n",
      "0 898.0 to\n",
      "0 898.0 is\n",
      "0 896.0 in\n",
      "0 880.0 that\n",
      "0 879.0 it\n",
      "0 873.0 with\n",
      "0 860.0 this\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 37123.0 the\n",
      "1 17778.0 and\n",
      "1 16692.0 of\n",
      "1 14798.0 to\n",
      "1 12885.0 is\n",
      "1 10535.0 in\n",
      "1 7467.0 it\n",
      "1 7282.0 that\n",
      "1 5785.0 as\n",
      "1 5273.0 with\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 31470.0 the\n",
      "0 14027.0 and\n",
      "0 13857.0 to\n",
      "0 13857.0 of\n",
      "0 10407.0 is\n",
      "0 9074.0 in\n",
      "0 7044.0 it\n",
      "0 6988.0 that\n",
      "0 4421.0 this\n",
      "0 4420.0 as\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_adj_adv = get_adj_adv(train)\n",
    "test_adj_adv = get_adj_adv(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train_adj_adv])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test_adj_adv])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 2666.0 not\n",
      "1 1633.0 more\n",
      "1 1316.0 so\n",
      "1 1247.0 most\n",
      "1 1200.0 just\n",
      "1 1111.0 good\n",
      "1 1072.0 also\n",
      "1 1045.0 even\n",
      "1 1027.0 very\n",
      "1 1014.0 only\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 2430.0 not\n",
      "0 1533.0 so\n",
      "0 1391.0 just\n",
      "0 1362.0 more\n",
      "0 1221.0 even\n",
      "0 1205.0 only\n",
      "0 1031.0 good\n",
      "0 935.0 bad\n",
      "0 915.0 much\n",
      "0 805.0 most\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenelisegardiner/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 22.344073604866896 movie\n",
      "1 18.75741514140869 like\n",
      "1 17.743868809614842 does\n",
      "1 16.896703821132636 story\n",
      "1 16.854901947165093 life\n",
      "1 16.76517275594625 just\n",
      "1 16.656367315117194 good\n",
      "1 16.355788961060934 time\n",
      "1 15.085297228325906 character\n",
      "1 14.486111572711723 best\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 27.247686886244978 movie\n",
      "0 21.205307710450015 like\n",
      "0 19.79476920211381 just\n",
      "0 18.706809390109427 bad\n",
      "0 17.641005245526756 does\n",
      "0 17.091878055954655 good\n",
      "0 16.611207729535327 plot\n",
      "0 16.55398161042137 time\n",
      "0 15.030208169308109 character\n",
      "0 14.87075481260817 story\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, ngram_range=(2,2))\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 845.0 of the\n",
      "1 787.0 in the\n",
      "1 682.0 the film\n",
      "1 657.0 to the\n",
      "1 634.0 and the\n",
      "1 577.0 to be\n",
      "1 570.0 on the\n",
      "1 533.0 with the\n",
      "1 521.0 for the\n",
      "1 515.0 is the\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 828.0 of the\n",
      "0 796.0 in the\n",
      "0 641.0 the film\n",
      "0 624.0 to be\n",
      "0 577.0 to the\n",
      "0 560.0 and the\n",
      "0 545.0 on the\n",
      "0 481.0 with the\n",
      "0 477.0 for the\n",
      "0 457.0 the movie\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "The best model for problem 1 is M3 with **87% accuracy**. M3 probably saw the highest performance given that most positive/negative words that would capture sentiment are adjectives and adverbs.\n",
    "\n",
    "When looking at the most informative words, M1, M2, and M5 do not have informative lists because stop words were not removed. Most of the informative words are just frequent words (e.g. of, the, and, that, it). M3 is slightly better because it captures words like good for positive and bad for negative, but they also share a lot of words. M4 is the best, which makes sense given it's formula emphasis on high value words, but again the positive and negative lists share words. This shared list of words indicated that we need to create a custom stop words list and run the experiments again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_neg_train, stem_neg_test = read_reviews('review_polarity.v2/txt_sentoken/neg', stem=True)\n",
    "stem_pos_train, stem_pos_test = read_reviews('review_polarity.v2/txt_sentoken/pos', stem=True)\n",
    "stem_pos_train_labels = [1 for i in range(len(stem_pos_train))]\n",
    "stem_neg_train_labels = [0 for i in range(len(stem_neg_train))]\n",
    "stem_pos_test_labels = [1 for i in range(len(stem_pos_test))]\n",
    "stem_neg_test_labels = [0 for i in range(len(stem_neg_test))]\n",
    "stem_train = stem_pos_train + stem_neg_train\n",
    "stem_test = stem_pos_test + stem_neg_test\n",
    "stem_train_labels = stem_pos_train_labels + stem_neg_train_labels\n",
    "stem_test_labels = stem_pos_test_labels + stem_neg_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "stem_train_features = vectorizer.fit_transform([' '.join(review) for review in stem_train])\n",
    "stem_test_features = vectorizer.transform([' '.join(review) for review in stem_test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(stem_train_features, stem_train_labels)\n",
    "predictions = nb_clf.predict(stem_test_features)\n",
    "accuracy = accuracy_score(stem_test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 900.0 the\n",
      "1 900.0 of\n",
      "1 899.0 to\n",
      "1 899.0 is\n",
      "1 899.0 and\n",
      "1 898.0 in\n",
      "1 894.0 it\n",
      "1 887.0 that\n",
      "1 879.0 with\n",
      "1 873.0 as\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 899.0 the\n",
      "0 899.0 of\n",
      "0 899.0 and\n",
      "0 898.0 to\n",
      "0 898.0 is\n",
      "0 896.0 in\n",
      "0 889.0 it\n",
      "0 880.0 that\n",
      "0 873.0 with\n",
      "0 860.0 thi\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "stem_train_features = vectorizer.fit_transform([' '.join(review) for review in stem_train])\n",
    "stem_test_features = vectorizer.transform([' '.join(review) for review in stem_test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(stem_train_features, stem_train_labels)\n",
    "predictions = nb_clf.predict(stem_test_features)\n",
    "accuracy = accuracy_score(stem_test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 37122.0 the\n",
      "1 17778.0 and\n",
      "1 16696.0 of\n",
      "1 14799.0 to\n",
      "1 12885.0 is\n",
      "1 10541.0 in\n",
      "1 8604.0 it\n",
      "1 7283.0 that\n",
      "1 5785.0 as\n",
      "1 5536.0 film\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 31470.0 the\n",
      "0 14027.0 and\n",
      "0 13858.0 of\n",
      "0 13857.0 to\n",
      "0 10407.0 is\n",
      "0 9085.0 in\n",
      "0 7949.0 it\n",
      "0 6991.0 that\n",
      "0 4512.0 film\n",
      "0 4423.0 thi\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_train_adj_adv = get_adj_adv(stem_train)\n",
    "stem_test_adj_adv = get_adj_adv(stem_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "stem_train_features = vectorizer.fit_transform([' '.join(review) for review in stem_train_adj_adv])\n",
    "stem_test_features = vectorizer.transform([' '.join(review) for review in stem_test_adj_adv])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(stem_train_features, stem_train_labels)\n",
    "predictions = nb_clf.predict(stem_test_features)\n",
    "accuracy = accuracy_score(stem_test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 2679.0 not\n",
      "1 1894.0 hi\n",
      "1 1634.0 more\n",
      "1 1293.0 so\n",
      "1 1248.0 most\n",
      "1 1240.0 thi\n",
      "1 1200.0 just\n",
      "1 1138.0 good\n",
      "1 1129.0 other\n",
      "1 1072.0 also\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 2434.0 not\n",
      "0 1495.0 so\n",
      "0 1391.0 just\n",
      "0 1369.0 hi\n",
      "0 1361.0 more\n",
      "0 1304.0 thi\n",
      "0 1241.0 even\n",
      "0 1051.0 good\n",
      "0 943.0 bad\n",
      "0 915.0 much\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurenelisegardiner/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.845"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8, stop_words='english', sublinear_tf=True)\n",
    "stem_train_features = vectorizer.fit_transform([' '.join(review) for review in stem_train])\n",
    "stem_test_features = vectorizer.transform([' '.join(review) for review in stem_test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(stem_train_features, stem_train_labels)\n",
    "predictions = nb_clf.predict(stem_test_features)\n",
    "accuracy = accuracy_score(stem_test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 23.28718465338969 wa\n",
      "1 21.123628977220868 charact\n",
      "1 21.028249830714365 like\n",
      "1 19.956752083075894 make\n",
      "1 19.730727920758845 time\n",
      "1 18.902287883181813 doe\n",
      "1 18.644755330309536 stori\n",
      "1 18.214209246092075 veri\n",
      "1 18.13782430551459 scene\n",
      "1 18.011183542812528 good\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 25.82322463722663 wa\n",
      "0 23.65874998551258 like\n",
      "0 21.59474054878139 charact\n",
      "0 21.032481785881604 just\n",
      "0 19.97537887820938 bad\n",
      "0 19.965091970184897 make\n",
      "0 19.837018532768607 onli\n",
      "0 19.170617578873305 time\n",
      "0 18.768513416423385 doe\n",
      "0 18.49071156796062 scene\n"
     ]
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True, ngram_range=(2,2))\n",
    "stem_train_features = vectorizer.fit_transform([' '.join(review) for review in stem_train])\n",
    "stem_test_features = vectorizer.transform([' '.join(review) for review in stem_test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(stem_train_features, stem_train_labels)\n",
    "predictions = nb_clf.predict(stem_test_features)\n",
    "accuracy = accuracy_score(stem_test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "The best model for problem 2 is M1 and M5 tied with **85% accuracy**. However, all models saw a drop in performance when using Porter stemming indicating we should not use it. \n",
    "\n",
    "When looking at the most informative words, M1 and M2 do not have informative lists because stop words were not removed. Most of the informative words are just frequent words (e.g. of, the, and, that, it). M3 is slightly better because it captures words like good for positive and bad for negative, but they also share a lot of words. M4 is the best, which makes sense given it's formula emphasis on high value words, but again the positive and negative lists share words. This shared list of words indicated that we need to create a custom stop words list and run the experiments again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRC_emotion = pd.read_csv(\"NRC_Emotion.txt\", sep=\"\\t\", skiprows=22, names=[\"Word\", \"Affect\", \"Indicator\"])\n",
    "NRC_emotion = NRC_emotion[NRC_emotion.Indicator==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratios = [calc_pos_neg(review, NRC_emotion) for review in train]\n",
    "test_ratios = [calc_pos_neg(review, NRC_emotion) for review in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(np.array(train_ratios).reshape(-1, 1), train_labels)\n",
    "predictions = nb_clf.predict(np.array(test_ratios).reshape(-1, 1))\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6467"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(NRC_emotion.Word.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "This model had the worst performance at **50% accuracy**. Given that the NRC Emotions only has 6,467 words with a labeled affect, the vocabulary is too small to capture the sentiment of a movie review. Additionally, the word `scary` may have a negative connotation in general, but may be a positive attribute when reviewing a horror film. Even with a larger vocabulary or more domain specific NRC Emotions dataset, reducing all the information from a review down to a single ratio may not capture enough of the signal in the data.\n",
    "\n",
    "# Question 4\n",
    "If I were to further iterate on this model, the first thing I would do is remove stop words from a custom stop word list. From looking at the signifcant words list, we can see they include a lot of English stop words, like `the` or `and`, or words that have no value in a movie review due to their high frequency, like `movie`, `film`, `story`, or `character`. As I would continue to iterate, I would add any words that appear in both the positive and negative lists to my custom stop words because they are clearly not indicative of sentiment. \n",
    "\n",
    "When vectorizing my models, I would also tune the `min_df` and `max_df` parameters. Given the assignment briefing I only set them on tf-idf and used the defaults on all the `CountVectorizer`. However, when playing with these parameters, I often saw a boost in performance on most models. As seen below, I was able to beat my best performance of **87% accuracy** to as high as **88% accuracy** by tuning my hyperparmeters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 1 M1\n",
    "vectorizer = CountVectorizer(binary=True, max_df=0.8, min_df=5)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 1 M3\n",
    "vectorizer = CountVectorizer(max_df=0.8, min_df=2)\n",
    "train_features = vectorizer.fit_transform([' '.join(review) for review in train_adj_adv])\n",
    "test_features = vectorizer.transform([' '.join(review) for review in test_adj_adv])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 2 M1\n",
    "vectorizer = CountVectorizer(binary=True, max_df=0.8, min_df=4)\n",
    "stem_train_features = vectorizer.fit_transform([' '.join(review) for review in stem_train])\n",
    "stem_test_features = vectorizer.transform([' '.join(review) for review in stem_test])\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(stem_train_features, stem_train_labels)\n",
    "predictions = nb_clf.predict(stem_test_features)\n",
    "accuracy = accuracy_score(stem_test_labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other experiments to try include normalizing frequency counts so review length does not have an impact. Review length wouldn't be more indicative of the writer's style than sentiment. I would also want to see the impact of lemmatization versus stemming or tokenization only. Finally, I would also want to experiment with pre-trained word vectors. I would expect a model incorporating word vectors would be the most performant given it's ability to capture semantic meaning, which is highly informative to sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
