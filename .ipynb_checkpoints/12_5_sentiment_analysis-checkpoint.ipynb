{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navie Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(c | x) = \\frac{P(x | c) P(c)}{P(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(C_k | x_1, x_2, ..., x_n) = \\frac{P(x_1, x_2, ..., x_n | C_k) P(C_k)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "Given Class $C_k$, $x_i$ is independant of $x_j$ when $i \\neq j$\n",
    "\n",
    "$P(C_k | x_1, x_2, ..., x_n) = \\frac{P(C_k) \\prod_{i=1}^nP(x_i | C_k)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "$P(x_1, x_2, ..., x_n)$ is the same for different class $k$s\n",
    "\n",
    "$P(C_k | x_1, x_2, ..., x_n) \\propto P(C_k) \\prod_{i=1}^nP(x_i | C_k)$\n",
    "\n",
    "Again, apply log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually split the dataset into train and test by 0.9 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "# read a pos/neg dir\n",
    "# each document is a review\n",
    "corpus_folder = './review_polarity/txt_sentoken/'\n",
    "def readDir(senti_folder, pattern, top_doc_num):\n",
    "    file_list = []\n",
    "    path_pattern = os.path.join(corpus_folder, senti_folder, pattern + '*.txt')\n",
    "    all_txt_paths = glob.glob(path_pattern)\n",
    "    # !!!!! I am only taking the first top_doc_num dcouments in pos/neg folder\n",
    "    for file_path in all_txt_paths[:top_doc_num]:\n",
    "        # print(file_path)\n",
    "        word_List = readFile(file_path)\n",
    "        # print(word_List)\n",
    "        file_list.append(word_List)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read one sample document\n",
    "def readFile(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        tokenzied_words = f.read().split()\n",
    "        return tokenzied_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_file_list = readDir('pos', 'cv[0-8]', top_doc_num = 90)\n",
    "train_neg_file_list = readDir('neg', 'cv[0-8]', top_doc_num = 90)\n",
    "train_pos_labels = [1 for i in range(len(train_pos_file_list))]\n",
    "train_neg_labels = [0 for i in range(len(train_neg_file_list))]\n",
    "\n",
    "test_pos_file_list = readDir('pos', 'cv9', top_doc_num = 10)\n",
    "test_neg_file_list = readDir('neg', 'cv9', top_doc_num = 10)\n",
    "test_pos_labels = [1 for i in range(len(test_pos_file_list))]\n",
    "test_neg_labels = [0 for i in range(len(test_neg_file_list))]\n",
    "\n",
    "train_file_list = train_pos_file_list + train_neg_file_list\n",
    "test_file_list = test_pos_file_list + test_neg_file_list\n",
    "\n",
    "train_labels = train_pos_labels + train_neg_labels\n",
    "test_labels = test_pos_labels + test_neg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.1, random_state = 0)\n",
    "# print(train_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create English stop words list (you can always define your own stopwords)\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word present/absense\n",
    "def clean(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    unique_punctuation_free = set(punctuation_free)\n",
    "    return ' '.join(punctuation_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# term frequency\n",
    "def clean1(tokenized):\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    return ' '.join(punctuation_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean2(tokenized):\n",
    "    # punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    # word_posTags = pos_tag(punctuation_free)\n",
    "    # get the POS tags\n",
    "    # pos_tags = [t[1] for t in word_posTags]\n",
    "    # ??? how to get adjv_words?\n",
    "    return ' '.join(adjv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus_clean is a list for tokenized documents\n",
    "# each list contains the string of the tokenized words in a document\n",
    "train_file_list_clean = [clean(doc) for doc in train_file_list]\n",
    "test_file_list_clean = [clean(doc) for doc in test_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CountVectorizer has a function to invoke bigram\n",
    "# However, think about how to use it in a correct way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sublinear tf-idf = (1 + log(tf)) x idf\n",
    "\n",
    "Check 11_7_tf_idf ~~ TF with Log Variant\n",
    "\n",
    "Or https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "What parameters you should use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer(min_df = 2, stop_words='english', sublinear_tf=True)\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokens = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # create a dataframe from a word matrix\n",
    "# def wm2df(wm, feat_names):\n",
    "    \n",
    "#     # create an index for each row\n",
    "#     doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
    "#     df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
    "#                       columns=feat_names)\n",
    "#     return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wm2df(train_features, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 5615)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = nb_clf.classes_\n",
    "print(class_labels)\n",
    "# coef_ just for the positive class\n",
    "nb_clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_most_informative_features(vectorizer, classifier, n=10):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in positive reviews\n",
      "1 3.485474104928371 film\n",
      "1 2.7786316707475223 movie\n",
      "1 2.1538134510442064 good\n",
      "1 2.115805364453226 like\n",
      "1 1.940998036559053 story\n",
      "1 1.9060070367569755 life\n",
      "1 1.85315053428588 time\n",
      "1 1.7799379088882974 just\n",
      "1 1.7351539214143994 does\n",
      "1 1.7247076555982765 films\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 3.4191328989873586 film\n",
      "0 3.3487884884628056 movie\n",
      "0 2.453029626130787 like\n",
      "0 2.2465396824527475 just\n",
      "0 2.1835677841571113 story\n",
      "0 2.028496773507254 bad\n",
      "0 1.9553805165118996 time\n",
      "0 1.889658169350273 character\n",
      "0 1.8744950379789147 characters\n",
      "0 1.8152638070975888 going\n"
     ]
    }
   ],
   "source": [
    "show_most_informative_features(vectorizer, nb_clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice, the following tools are friendly to Mac, Linux and C++ users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone http://github.com/stanfordnlp/glove\n",
    "\n",
    "cd glove && make\n",
    "\n",
    "./demo.sh\n",
    "\n",
    "python eval/python/evaluate.py\n",
    "\n",
    "python eval/python/distance.py\n",
    "\n",
    "python eval/python/word_analogy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "git clone https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "cd fastText\n",
    "\n",
    "mkdir build && cd build && cmake ..\n",
    "\n",
    "make && make install\n",
    "\n",
    "./fasttext skipgram -input data.txt -output model\n",
    "\n",
    "cat queries.txt | ./fasttext print-word-vectors model.bin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
