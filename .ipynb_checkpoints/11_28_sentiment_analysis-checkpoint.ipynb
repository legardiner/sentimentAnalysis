{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navie Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(c | x) = \\frac{P(x | c) P(c)}{P(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(C_k | x_1, x_2, ..., x_n) = \\frac{P(x_1, x_2, ..., x_n | C_k) P(C_k)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "Given Class $C_k$, $x_i$ is independant of $x_j$ when $i \\neq j$\n",
    "\n",
    "$P(C_k | x_1, x_2, ..., x_n) = \\frac{P(C_k) \\prod_{i=1}^nP(x_i | C_k)}{P(x_1, x_2, ..., x_n)}$\n",
    "\n",
    "$P(x_1, x_2, ..., x_n)$ is the same for different class $k$s\n",
    "\n",
    "$P(C_k | x_1, x_2, ..., x_n) \\propto P(C_k) \\prod_{i=1}^nP(x_i | C_k)$\n",
    "\n",
    "Again, apply log probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read one sample document\n",
    "def readFile(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a sample corpus\n",
    "positive_lines = readFile('./review_polarity/txt_sentoken/pos/cv000_29590.txt')\n",
    "positive_labels = [1 for i in range(len(positive_lines))]\n",
    "negative_lines = readFile('./review_polarity/txt_sentoken/neg/cv000_29416.txt')\n",
    "negative_labels = [0 for i in range(len(negative_lines))]\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_list = positive_lines + negative_lines\n",
    "senti_labels = positive_labels + negative_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_list\n",
    "#senti_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create English stop words list (you can always define your own stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to remove stop words from sentences & lemmatize verbs and nouns. \n",
    "def clean1(doc):\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    stop_free = [x.translate(translator) for x in tokenized]\n",
    "    #stop_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x) and x not in stop_words]\n",
    "    lemma_verb = [lemmatizer.lemmatize(word,'v') for word in stop_free]\n",
    "    lemma_noun = [lemmatizer.lemmatize(word,'n') for word in lemma_verb]\n",
    "    y = [s for s in lemma_noun if len(s) > 2]\n",
    "    return ' '.join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "def clean(doc):\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    # Apply POS tagging after removing punctuations\n",
    "    word_posTags = pos_tag(punctuation_free)\n",
    "    stop_free = [x for x in punctuation_free if x not in stop_words]\n",
    "    # Apply POS tagging after removing stop words\n",
    "    # word_posTags = pos_tag(stop_free)\n",
    "    pos_tags = [t[1] for t in word_posTags]    \n",
    "    lemma_verb = [lemmatizer.lemmatize(word,'v') for word in stop_free]\n",
    "    lemma_noun = [lemmatizer.lemmatize(word,'n') for word in lemma_verb]\n",
    "    y = [s for s in lemma_noun if len(s) > 2]\n",
    "    # add document len feature\n",
    "    # ' '.join(y) + ' ' + len(y)\n",
    "    # add pos tag features\n",
    "    return ' '.join(y + pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test DT VBZ DT NN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean('This is a test.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nlp.stanford.edu/software/\n",
    "How would you add other featuees such as DependencyParser features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_jar = '/Users/xiaofengzhu/Downloads/stanford_models/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/xiaofengzhu/Downloads/stanford_models/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('shot', 'VBD'), 'nsubj', ('I', 'PRP')),\n",
       " (('shot', 'VBD'), 'dobj', ('elephant', 'NN')),\n",
       " (('elephant', 'NN'), 'det', ('an', 'DT')),\n",
       " (('shot', 'VBD'), 'prep', ('in', 'IN')),\n",
       " (('in', 'IN'), 'pobj', ('pajamas', 'NNS')),\n",
       " (('pajamas', 'NNS'), 'poss', ('my', 'PRP$'))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.raw_parse('I shot an elephant in my pajamas')\n",
    "dep = next(result)\n",
    "\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try four choices in the following list\n",
    "- tokenization: white space vs. treebank-style vs. sentiment-aware style\n",
    "\n",
    "- stemming /lemmatization\n",
    "\n",
    "- negation / negation scope\n",
    "\n",
    "- POS tagging\n",
    "\n",
    "- dependency parsing\n",
    "\n",
    "- text length\n",
    "\n",
    "- pos/neg word ratio (i.e., the ratio of pos to neg words as defiend by a quality sentiment lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus_clean is a list for tokenized documents (a list of list)\n",
    "# each list contains the tokenized words in a document\n",
    "corpus_clean = [clean(doc.strip()) for doc in doc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#corpus_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# # if you want to use Pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# nb_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "train_features = vectorizer.fit_transform([doc for doc in corpus_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<60x528 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1328 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # sparse matrix\n",
    "#train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in the past\n",
    "# nb_clf.fit(train_features, senti_labels)\n",
    "# predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables for average classification report\n",
    "originalclass = []\n",
    "predictedclass = []\n",
    "\n",
    "#Make our customer score\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    originalclass.extend(y_true)\n",
    "    predictedclass.extend(y_pred)\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.94      0.81        35\n",
      "          1       0.86      0.48      0.62        25\n",
      "\n",
      "avg / total       0.78      0.75      0.73        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# the number of folds you want to set\n",
    "NUM_FOLDS = 3\n",
    "results = cross_val_score(nb_clf, train_features, senti_labels, \\\n",
    "          cv=NUM_FOLDS, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "\n",
    "# Average values in classification report for all folds in a K-fold Cross-validation  \n",
    "print(classification_report(originalclass, predictedclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[-1.1493862e-03 -3.5563384e-03 -3.3391684e-03 -5.0757156e-04\n",
      "  1.1734351e-03  9.1065059e-04  7.9620909e-04  1.9075215e-03\n",
      " -2.2747642e-03  3.3968594e-03  3.3861948e-03  3.0182032e-03\n",
      " -7.4280753e-05 -3.6991336e-03 -7.9530553e-04 -4.3100165e-03\n",
      " -4.5402953e-03 -4.6887724e-03 -2.1508015e-03  2.7396295e-03\n",
      "  1.4228281e-03 -1.6857482e-03  7.2761270e-04 -4.9594543e-03\n",
      "  6.2634121e-04  2.5783337e-04  1.3557264e-03  1.7752484e-03\n",
      " -8.8586094e-04  7.7218033e-04  4.1525191e-04 -1.9109838e-03\n",
      " -3.1143862e-03 -2.5595869e-03 -1.5352821e-03  3.9402780e-04\n",
      "  1.5057779e-03  3.7732718e-03 -3.6545335e-03  2.2936778e-03\n",
      "  2.3889232e-03  4.0901573e-03 -4.0119444e-03  4.4486411e-03\n",
      "  4.8838388e-03 -3.3961335e-04 -1.0031519e-03  2.4855658e-03\n",
      " -9.1987365e-04  3.9645368e-03 -1.1648053e-03 -4.6440419e-03\n",
      "  1.1779242e-03  1.9377724e-03  3.8601507e-03  3.8058308e-03\n",
      " -4.4489228e-03  1.0616251e-03  3.9145038e-03 -1.0056788e-03\n",
      "  1.7377589e-03  3.4984176e-03  5.1571627e-04 -4.8866142e-03\n",
      "  3.9911192e-04  2.4871861e-03  2.3124861e-05 -2.4601568e-03\n",
      " -3.0835462e-03  1.9299593e-03  2.7723897e-03  4.6211262e-03\n",
      " -4.1078157e-03 -4.0057343e-03  3.9384519e-03 -3.8245700e-05\n",
      "  1.7726412e-03 -1.2302134e-03  1.9867858e-03  4.1821278e-03\n",
      " -3.0664077e-03 -9.1029023e-04  5.9385895e-04  3.3060287e-03\n",
      "  2.7572548e-03 -3.5527602e-03  3.2314465e-03  4.4603697e-03\n",
      " -3.6411823e-03  1.3809965e-03  4.8441957e-03  1.9096923e-03\n",
      " -4.2292168e-03  1.2788954e-03 -2.7675536e-03 -1.7410489e-03\n",
      " -1.0273939e-03 -4.4691525e-03 -4.8480751e-03  2.4805458e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaofengzhu/conda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# define training data\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "# train model\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['word2vec'])\n",
    "# save model\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaofengzhu/conda/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('and', 0.22534842789173126), ('another', 0.17810091376304626)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.most_similar(positive='one', topn=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaofengzhu/conda/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.similarity(w1='is', w2='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # pip install glove-python\n",
    "# # https://github.com/maciejkula/glove-python\n",
    "# from glove import Corpus, Glove\n",
    "# corpus = Corpus()\n",
    "# corpus.fit(sentences, window=10)\n",
    "# glove = Glove(no_components=100, learning_rate=0.05)\n",
    "# glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "# glove.add_dictionary(corpus.dictionary)\n",
    "# glove.most_similar('man')\n",
    "# glove.most_similar('man', number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
